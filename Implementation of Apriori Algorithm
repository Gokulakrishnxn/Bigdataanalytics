import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
import networkx as nx
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Set visualization style
sns.set_style("whitegrid")
plt.rcParams['figure.facecolor'] = 'white'

print("=" * 120)
print("APRIORI ALGORITHM IMPLEMENTATION - MARKET BASKET ANALYSIS FOR BIG DATA")
print("=" * 120)

# Generate realistic grocery store transaction dataset
np.random.seed(42)

# Define product catalog with categories
products = {
    'Dairy': ['whole milk', 'yogurt', 'butter', 'cream cheese', 'whipped cream'],
    'Bakery': ['rolls/buns', 'bread', 'pastry', 'croissant', 'cake'],
    'Produce': ['tropical fruit', 'root vegetables', 'other vegetables', 'citrus fruit', 'berries'],
    'Beverages': ['soda', 'bottled water', 'coffee', 'tea', 'fruit juice'],
    'Meat': ['beef', 'chicken', 'sausage', 'pork', 'ham'],
    'Snacks': ['chocolate', 'candy', 'chips', 'cookies', 'popcorn'],
    'Pantry': ['rice', 'pasta', 'canned goods', 'cereal', 'oil']
}

# Flatten product list
all_products = [item for category in products.values() for item in category]

# Popular product combinations (simulating real shopping patterns)
common_combinations = [
    ['whole milk', 'bread', 'butter'],
    ['whole milk', 'yogurt', 'tropical fruit'],
    ['rolls/buns', 'sausage', 'root vegetables'],
    ['soda', 'chips', 'chocolate'],
    ['coffee', 'pastry', 'cream cheese'],
    ['chicken', 'rice', 'other vegetables'],
    ['beef', 'root vegetables', 'rolls/buns'],
    ['yogurt', 'berries', 'cereal'],
    ['pasta', 'canned goods', 'other vegetables'],
    ['bottled water', 'tropical fruit', 'yogurt']
]

# Generate transactions
n_transactions = 9835
transactions = []

for i in range(n_transactions):
    if np.random.random() < 0.3:  # 30% chance of common combination
        base_items = list(np.random.choice(common_combinations))
    else:
        # Random selection
        n_items = np.random.choice([1, 2, 3, 4, 5, 6], p=[0.15, 0.25, 0.25, 0.20, 0.10, 0.05])
        base_items = list(np.random.choice(all_products, size=n_items, replace=False))
    
    # Add occasional random items
    if np.random.random() < 0.4:
        extra_items = list(np.random.choice(all_products, size=np.random.randint(1, 3), replace=False))
        base_items.extend(extra_items)
    
    # Remove duplicates
    base_items = list(set(base_items))
    transactions.append(base_items)

print("\n1. DATASET OVERVIEW")
print("-" * 120)
print(f"Total Transactions: {len(transactions)}")
print(f"Unique Products: {len(all_products)}")
print(f"Average Items per Transaction: {np.mean([len(t) for t in transactions]):.2f}")
print(f"Max Items in Transaction: {max([len(t) for t in transactions])}")
print(f"Min Items in Transaction: {min([len(t) for t in transactions])}")

print("\n2. SAMPLE TRANSACTIONS")
print("-" * 120)
for i in range(5):
    print(f"Transaction {i+1}: {transactions[i]}")

# Analyze transaction sizes
transaction_sizes = [len(t) for t in transactions]
size_distribution = Counter(transaction_sizes)

print("\n3. TRANSACTION SIZE DISTRIBUTION")
print("-" * 120)
for size in sorted(size_distribution.keys()):
    count = size_distribution[size]
    percentage = (count / len(transactions)) * 100
    print(f"  {size} items: {count} transactions ({percentage:.2f}%)")

# Most frequent individual items
all_items_flat = [item for transaction in transactions for item in transaction]
item_frequency = Counter(all_items_flat)
most_common_items = item_frequency.most_common(15)

print("\n4. TOP 15 MOST PURCHASED ITEMS")
print("-" * 120)
print(f"{'Rank':<6} {'Product':<25} {'Frequency':<12} {'Support (%)'}")
print("-" * 70)
for rank, (item, freq) in enumerate(most_common_items, 1):
    support = (freq / len(transactions)) * 100
    print(f"{rank:<6} {item:<25} {freq:<12} {support:.2f}%")

# ==================== TRANSACTION ENCODING ====================
print("\n5. DATA PREPROCESSING - TRANSACTION ENCODING")
print("-" * 120)

te = TransactionEncoder()
te_array = te.fit(transactions).transform(transactions)
df_encoded = pd.DataFrame(te_array, columns=te.columns_)

print(f"Encoded dataset shape: {df_encoded.shape}")
print(f"Data type: Boolean matrix")
print(f"Memory usage: {df_encoded.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
print("\nFirst 5 transactions (encoded):")
print(df_encoded.head().to_string())

# ==================== APRIORI ALGORITHM - FREQUENT ITEMSETS ====================
print("\n6. APRIORI ALGORITHM - MINING FREQUENT ITEMSETS")
print("-" * 120)

min_support = 0.01  # 1% minimum support
print(f"Minimum Support Threshold: {min_support*100}%")
print(f"This means items must appear in at least {int(min_support*len(transactions))} transactions")

# Apply Apriori algorithm
frequent_itemsets = apriori(df_encoded, min_support=min_support, use_colnames=True)
frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))
frequent_itemsets = frequent_itemsets.sort_values('support', ascending=False).reset_index(drop=True)

print(f"\nTotal Frequent Itemsets Found: {len(frequent_itemsets)}")
print(f"  1-itemsets: {len(frequent_itemsets[frequent_itemsets['length'] == 1])}")
print(f"  2-itemsets: {len(frequent_itemsets[frequent_itemsets['length'] == 2])}")
print(f"  3-itemsets: {len(frequent_itemsets[frequent_itemsets['length'] == 3])}")
print(f"  4+ itemsets: {len(frequent_itemsets[frequent_itemsets['length'] >= 4])}")

print("\n7. TOP 20 FREQUENT ITEMSETS")
print("-" * 120)
print(frequent_itemsets.head(20).to_string(index=False))

# ==================== ASSOCIATION RULES GENERATION ====================
print("\n8. ASSOCIATION RULES GENERATION")
print("-" * 120)

min_threshold = 0.3  # Minimum confidence threshold
print(f"Minimum Confidence Threshold: {min_threshold*100}%")

# Generate association rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=min_threshold)
rules = rules.sort_values('lift', ascending=False).reset_index(drop=True)

print(f"\nTotal Association Rules Generated: {len(rules)}")

# Add rule interpretation
rules['antecedents_str'] = rules['antecedents'].apply(lambda x: ', '.join(list(x)))
rules['consequents_str'] = rules['consequents'].apply(lambda x: ', '.join(list(x)))

print("\n9. ASSOCIATION RULES METRICS EXPLAINED")
print("-" * 120)
print("Support: Frequency of itemset in all transactions (popularity)")
print("Confidence: Probability of consequent given antecedent (reliability)")
print("Lift: Ratio of observed support to expected support (strength of association)")
print("  Lift > 1: Positive correlation (items bought together more than expected)")
print("  Lift = 1: No correlation (independent)")
print("  Lift < 1: Negative correlation (items substitute each other)")

print("\n10. TOP 20 ASSOCIATION RULES (SORTED BY LIFT)")
print("-" * 120)
display_cols = ['antecedents_str', 'consequents_str', 'support', 'confidence', 'lift']
top_rules = rules[display_cols].head(20)
print(top_rules.to_string(index=False))

# ==================== RULE FILTERING AND ANALYSIS ====================
print("\n11. HIGH-CONFIDENCE RULES (Confidence > 60%)")
print("-" * 120)
high_confidence_rules = rules[rules['confidence'] > 0.6][display_cols].head(15)
print(high_confidence_rules.to_string(index=False))

print("\n12. STRONG ASSOCIATION RULES (Lift > 2.0)")
print("-" * 120)
strong_lift_rules = rules[rules['lift'] > 2.0][display_cols].head(15)
print(strong_lift_rules.to_string(index=False))

# Statistical summary of rules
print("\n13. ASSOCIATION RULES STATISTICAL SUMMARY")
print("-" * 120)
print(f"Support Statistics:")
print(f"  Mean: {rules['support'].mean():.4f}")
print(f"  Median: {rules['support'].median():.4f}")
print(f"  Min: {rules['support'].min():.4f}")
print(f"  Max: {rules['support'].max():.4f}")

print(f"\nConfidence Statistics:")
print(f"  Mean: {rules['confidence'].mean():.4f}")
print(f"  Median: {rules['confidence'].median():.4f}")
print(f"  Min: {rules['confidence'].min():.4f}")
print(f"  Max: {rules['confidence'].max():.4f}")

print(f"\nLift Statistics:")
print(f"  Mean: {rules['lift'].mean():.4f}")
print(f"  Median: {rules['lift'].median():.4f}")
print(f"  Min: {rules['lift'].min():.4f}")
print(f"  Max: {rules['lift'].max():.4f}")

# ==================== COMPREHENSIVE VISUALIZATIONS ====================
fig = plt.figure(figsize=(20, 16))

# Plot 1: Transaction Size Distribution
ax1 = plt.subplot(3, 3, 1)
sizes = sorted(size_distribution.keys())
counts = [size_distribution[s] for s in sizes]
bars = ax1.bar(sizes, counts, color='skyblue', edgecolor='black')
ax1.set_xlabel('Number of Items', fontsize=10)
ax1.set_ylabel('Number of Transactions', fontsize=10)
ax1.set_title('1. Transaction Size Distribution', fontsize=12, fontweight='bold')
ax1.grid(axis='y', alpha=0.3)
for bar, count in zip(bars, counts):
    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
            f'{count}', ha='center', va='bottom', fontsize=8)

# Plot 2: Top 15 Most Frequent Items
ax2 = plt.subplot(3, 3, 2)
items = [item for item, _ in most_common_items]
frequencies = [freq for _, freq in most_common_items]
ax2.barh(range(len(items)), frequencies, color='coral', edgecolor='black')
ax2.set_yticks(range(len(items)))
ax2.set_yticklabels(items, fontsize=8)
ax2.set_xlabel('Frequency', fontsize=10)
ax2.set_title('2. Top 15 Most Purchased Items', fontsize=12, fontweight='bold')
ax2.invert_yaxis()
ax2.grid(axis='x', alpha=0.3)

# Plot 3: Frequent Itemsets by Length
ax3 = plt.subplot(3, 3, 3)
itemset_lengths = frequent_itemsets['length'].value_counts().sort_index()
bars = ax3.bar(itemset_lengths.index, itemset_lengths.values, color='lightgreen', edgecolor='black')
ax3.set_xlabel('Itemset Length', fontsize=10)
ax3.set_ylabel('Count', fontsize=10)
ax3.set_title('3. Frequent Itemsets by Length', fontsize=12, fontweight='bold')
ax3.grid(axis='y', alpha=0.3)
for bar, count in zip(bars, itemset_lengths.values):
    ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
            f'{count}', ha='center', va='bottom', fontsize=9)

# Plot 4: Support vs Confidence Scatter
ax4 = plt.subplot(3, 3, 4)
scatter = ax4.scatter(rules['support'], rules['confidence'], c=rules['lift'], 
                     cmap='viridis', s=50, alpha=0.6, edgecolors='black')
ax4.set_xlabel('Support', fontsize=10)
ax4.set_ylabel('Confidence', fontsize=10)
ax4.set_title('4. Support vs Confidence (Color: Lift)', fontsize=12, fontweight='bold')
ax4.grid(True, alpha=0.3)
plt.colorbar(scatter, ax=ax4, label='Lift')

# Plot 5: Lift Distribution
ax5 = plt.subplot(3, 3, 5)
ax5.hist(rules['lift'], bins=30, color='mediumpurple', edgecolor='black', alpha=0.7)
ax5.axvline(rules['lift'].mean(), color='red', linestyle='--', linewidth=2, 
           label=f'Mean: {rules["lift"].mean():.2f}')
ax5.axvline(1, color='green', linestyle='--', linewidth=2, label='Lift=1 (Independent)')
ax5.set_xlabel('Lift', fontsize=10)
ax5.set_ylabel('Frequency', fontsize=10)
ax5.set_title('5. Lift Distribution', fontsize=12, fontweight='bold')
ax5.legend(fontsize=8)
ax5.grid(axis='y', alpha=0.3)

# Plot 6: Confidence Distribution
ax6 = plt.subplot(3, 3, 6)
ax6.hist(rules['confidence'], bins=30, color='lightcoral', edgecolor='black', alpha=0.7)
ax6.axvline(rules['confidence'].mean(), color='red', linestyle='--', linewidth=2,
           label=f'Mean: {rules["confidence"].mean():.2f}')
ax6.set_xlabel('Confidence', fontsize=10)
ax6.set_ylabel('Frequency', fontsize=10)
ax6.set_title('6. Confidence Distribution', fontsize=12, fontweight='bold')
ax6.legend(fontsize=8)
ax6.grid(axis='y', alpha=0.3)

# Plot 7: Top Rules by Lift
ax7 = plt.subplot(3, 3, 7)
top_lift_rules = rules.head(10)
rule_labels = [f"{list(row['antecedents'])[0][:15]} → {list(row['consequents'])[0][:15]}" 
               for _, row in top_lift_rules.iterrows()]
ax7.barh(range(len(rule_labels)), top_lift_rules['lift'], color='gold', edgecolor='black')
ax7.set_yticks(range(len(rule_labels)))
ax7.set_yticklabels(rule_labels, fontsize=7)
ax7.set_xlabel('Lift', fontsize=10)
ax7.set_title('7. Top 10 Rules by Lift', fontsize=12, fontweight='bold')
ax7.invert_yaxis()
ax7.grid(axis='x', alpha=0.3)

# Plot 8: Association Network (Top Rules)
ax8 = plt.subplot(3, 3, 8)
G = nx.DiGraph()
top_network_rules = rules.head(15)
for _, row in top_network_rules.iterrows():
    antecedents = list(row['antecedents'])
    consequents = list(row['consequents'])
    for ant in antecedents:
        for cons in consequents:
            G.add_edge(ant[:12], cons[:12], weight=row['lift'])

pos = nx.spring_layout(G, k=0.5, iterations=50)
edges = G.edges()
weights = [G[u][v]['weight'] for u, v in edges]
nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=800, ax=ax8)
nx.draw_networkx_labels(G, pos, font_size=7, ax=ax8)
nx.draw_networkx_edges(G, pos, width=[w*0.5 for w in weights], 
                       edge_color='gray', arrows=True, 
                       arrowsize=15, ax=ax8, alpha=0.6)
ax8.set_title('8. Association Network (Top 15 Rules)', fontsize=12, fontweight='bold')
ax8.axis('off')

# Plot 9: Rules Count by Antecedent Size
ax9 = plt.subplot(3, 3, 9)
rules['antecedent_len'] = rules['antecedents'].apply(lambda x: len(x))
rules['consequent_len'] = rules['consequents'].apply(lambda x: len(x))
ant_counts = rules['antecedent_len'].value_counts().sort_index()
bars = ax9.bar(ant_counts.index, ant_counts.values, color='steelblue', edgecolor='black')
ax9.set_xlabel('Antecedent Size', fontsize=10)
ax9.set_ylabel('Number of Rules', fontsize=10)
ax9.set_title('9. Rules Distribution by Antecedent Size', fontsize=12, fontweight='bold')
ax9.grid(axis='y', alpha=0.3)
for bar, count in zip(bars, ant_counts.values):
    ax9.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
            f'{count}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.savefig('apriori_comprehensive_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# ==================== BUSINESS RECOMMENDATIONS ====================
print("\n14. STRATEGIC BUSINESS RECOMMENDATIONS")
print("-" * 120)

print("\nA. PRODUCT PLACEMENT STRATEGIES:")
top_associations = rules.head(5)
for idx, row in top_associations.iterrows():
    ant = ', '.join(list(row['antecedents']))
    cons = ', '.join(list(row['consequents']))
    print(f"  • Place {cons} near {ant}")
    print(f"    Confidence: {row['confidence']*100:.1f}%, Lift: {row['lift']:.2f}")

print("\nB. BUNDLE OFFERS:")
bundle_rules = rules[rules['confidence'] > 0.5].head(5)
for idx, row in bundle_rules.iterrows():
    ant = ', '.join(list(row['antecedents']))
    cons = ', '.join(list(row['consequents']))
    print(f"  • Create bundle: {ant} + {cons}")
    print(f"    Expected conversion: {row['confidence']*100:.1f}%")

print("\nC. CROSS-SELLING OPPORTUNITIES:")
cross_sell = rules[(rules['lift'] > 1.5) & (rules['confidence'] > 0.4)].head(5)
for idx, row in cross_sell.iterrows():
    ant = ', '.join(list(row['antecedents']))
    cons = ', '.join(list(row['consequents']))
    print(f"  • When customer buys {ant}, recommend {cons}")
    print(f"    Success probability: {row['confidence']*100:.1f}%")

print("\nD. PROMOTIONAL CAMPAIGNS:")
print(f"  • Focus promotions on top {len(most_common_items)} products")
print(f"  • Target customers with {int(rules['antecedent_len'].mean())} complementary items")
print(f"  • Average basket size optimization: {np.mean([len(t) for t in transactions]):.1f} items")

# ==================== RULE EXAMPLES ====================
print("\n15. PRACTICAL RULE EXAMPLES")
print("-" * 120)

sample_rules = rules.head(10)
for idx, row in sample_rules.iterrows():
    ant = ', '.join(list(row['antecedents']))
    cons = ', '.join(list(row['consequents']))
    print(f"\nRule {idx+1}: IF customer buys [{ant}] THEN they buy [{cons}]")
    print(f"  Support: {row['support']*100:.2f}% (appears in {int(row['support']*len(transactions))} transactions)")
    print(f"  Confidence: {row['confidence']*100:.2f}% ({row['confidence']*100:.1f}% of customers who buy antecedent also buy consequent)")
    print(f"  Lift: {row['lift']:.2f} ({row['lift']:.1f}x more likely than random)")

print("\n" + "=" * 120)
print("APRIORI ALGORITHM ANALYSIS COMPLETE")
print("=" * 120)
print("\nVisualization saved as 'apriori_comprehensive_analysis.png'")
print("\nKey Achievements:")
print(f"  • Processed {len(transactions):,} transactions with {len(all_products)} unique products")
print(f"  • Discovered {len(frequent_itemsets)} frequent itemsets using Apriori algorithm")
print(f"  • Generated {len(rules)} association rules for business intelligence")
print(f"  • Identified top product combinations with lift up to {rules['lift'].max():.2f}")
print("  • Provided actionable recommendations for product placement and bundling")
print("  • Created visual network of product associations for strategic planning")
